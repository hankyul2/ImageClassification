Timm Library 구현체와 차이점 
0. linear layer projection -> conv layer projection : 변경
1. droputout -> drop path : 무시
2. relu -> gelu : 변경
3. representation layer를 추가하는 경우도 고려 : 무시
4. norm layer를 LayerNorm이 아닌 경우도 고려 : 무시
5. distillation을 하는 경우도 고려 : 무시
6. timm library weight init method :
- conv/weight, bias : kaiming_uniform_ -> lecun_normal, zero
- cls token : uniform -> trunc_normal_(std=0.2)
- pos_embeding : uniform -> trucn_normal_(std=0.2)
	- q, k, v, linear_projection/weight, bias : xavier_uniform,zero -> trunc_normal, zero
	- w1, w2/weight, bias :  xavier_uniform,zero -> trunc_normal, zero
	- norm_layer/a_2, b_2 : zero, one
- norm_layer/a_2, b_2 : zero, one
- mlp_head/weight, bias : xavier_uniform,zero -> zero, zero
*trunc_normal_() is re-sampling outside values repeatly by normal pdf* 

model_weight
model: 
r50+vit-b_16_224, r50+vit-l_32_224,
vit-b_16_224, vit-b_32_224
vit_l_16_224, vit_l_32_224
pretrained on: imagenet21k, imagenet

imagenet21k pretrained 된 모델 가지고 imagenet, cifar10, cifar100에서 성능 내보기
goal : dropout=0.1 일 때의 성능 나오게 하기
공식 학습 결과 : https://github.com/google-research/vision_transformer
공식 학습 방법 : https://github.com/google-research/vision_transformer#fine-tuning-a-model

gpu memory usage
vit_base_patch16_224 : 4
vit_large_patch16_224 : 6
vit_base_patch32_224 : 3
vit_large_patch32_224 : 5


추가 도전 : imageNet을 384x384로 학습했을 때의 성능 변화 관찰하기
내가 이해한 2D interpolation의 의미
모델을 초기화 할 때 224x224 사이즈로 학습된 pe를 384x384로 늘여뜨린다?
